{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-lX2LNl-bZ6"
      },
      "source": [
        "# An√°lisis de Retenci√≥n de Clientes (CHURN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBpI7IH--c0c"
      },
      "source": [
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF2frQQR-iK9"
      },
      "source": [
        "## Contexto del Negocio\n",
        "\n",
        "El banco ficticio Monopoly, con a√±os de operaci√≥n en Chile, ha sido recientemente adquirido por una entidad extranjera: el banco Dormammu.\n",
        "\n",
        "Como parte del proceso de integraci√≥n, Dormammu ha solicitado un an√°lisis exhaustivo de la base de datos de clientes de Monopoly ‚Äîque contiene 574 variables y 51.124 registros‚Äî con el objetivo de:\n",
        "\n",
        "* Comprender el comportamiento financiero de los clientes actuales.\n",
        "* Identificar patrones de uso en productos como cuentas corrientes y tarjetas de cr√©dito.\n",
        "* Detectar oportunidades para mejorar los servicios existentes y aumentar la rentabilidad.\n",
        "* Evaluar estrategias para retener clientes actuales y atraer nuevos usuarios hacia los productos del banco fusionado.\n",
        "\n",
        "Este an√°lisis ser√° clave para dise√±ar pol√≠ticas comerciales, campa√±as de fidelizaci√≥n, y planes de producto m√°s alineados con los perfiles y necesidades detectadas en la base de clientes.\n",
        "\n",
        "**Objetivo del An√°lisis**:\n",
        "\n",
        "**Retenci√≥n de Clientes**: Desarrollar un modelo predictivo que permita identificar clientes con riesgo de abandonar el banco. El objetivo es facilitar la creaci√≥n de campa√±as de retenci√≥n proactivas y focalizadas, basadas en patrones de comportamiento reales.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55nFrsv0-jzB"
      },
      "outputs": [],
      "source": [
        "#importaciones de librer√≠as\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub\n",
        "import joblib\n",
        "from seaborn import load_dataset\n",
        "from seaborn import kdeplot\n",
        "from seaborn import lmplot\n",
        "from seaborn import boxplot\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from scipy.sparse import hstack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configuraci√≥n del Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skRDGggIcNT_",
        "outputId": "ab9ce46b-55b4-413e-fbf5-15d8c388ecef"
      },
      "outputs": [],
      "source": [
        "# Descargar dataset\n",
        "path = kagglehub.dataset_download(\"nadiaarellanog/base-clientes-monopoly\")\n",
        "\n",
        "print(\"Ruta:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48eTrBvLcUtD",
        "outputId": "8f64f2f0-6167-453f-83f8-6ee1d1653786"
      },
      "outputs": [],
      "source": [
        "monopoly = pd.read_csv(path + \"/Base_clientes_Monopoly.csv\", delimiter='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csT3yh6e-k-Z",
        "outputId": "7ad0cff1-3237-4794-9241-62cfcaae1be5"
      },
      "outputs": [],
      "source": [
        "# Mostrar las primeras filas del DataFrame para verificar la carga del archivo\n",
        "print(monopoly.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjoeTPXS-pJW"
      },
      "source": [
        "### Comprensi√≥n de los Datos\n",
        "\n",
        "El conjunto de datos proporcionado por el banco Monopoly (ahora parte de Dormammu) contiene informaci√≥n detallada de m√°s de 51.000 clientes y 574 variables, abarcando:\n",
        "\n",
        "* Datos demogr√°ficos\n",
        "* Informaci√≥n sobre productos financieros (cuentas, tarjetas, cr√©ditos)\n",
        "* Actividad transaccional mensual (volumen y frecuencia)\n",
        "* Indicadores de comportamiento y engagement\n",
        "\n",
        "Este dataset permitir√° construir una visi√≥n rica y detallada de cada cliente, as√≠ como modelar su propensi√≥n a continuar utilizando los servicios del banco.\n",
        "\n",
        "\n",
        "### Objetivos de esta Fase\n",
        "\n",
        "Durante la comprensi√≥n de los datos se busca:\n",
        "\n",
        "* Entender la estructura y significado de las variables disponibles, incluyendo unidades, escalas y formatos.\n",
        "* Detectar problemas de calidad de datos: valores faltantes, formatos inconsistentes, tipos incorrectos o valores at√≠picos.\n",
        "* Identificar variables relevantes para el modelo de retenci√≥n, bas√°ndose en su naturaleza, correlaci√≥n con el target y distribuci√≥n.\n",
        "\n",
        "\n",
        "### Descripci√≥n de Variables Relevantes\n",
        "\n",
        "A continuaci√≥n se resumen algunas de las variables clave agrupadas por tipo, destacando su utilidad en el an√°lisis:\n",
        "\n",
        "üîπ Variables Demogr√°ficas\n",
        "* Regi√≥n: Zona geogr√°fica de residencia del cliente. √ötil para segmentaci√≥n geogr√°fica.\n",
        "* Edad: Factor clave en el comportamiento financiero y tipo de productos utilizados.\n",
        "* Sexo: Potencialmente relevante para personalizaci√≥n de estrategias.\n",
        "* Renta: Indicador del poder adquisitivo del cliente, relacionado con su volumen de uso de productos.\n",
        "\n",
        "üîπ Productos Financieros\n",
        "* Ctacte: Tenencia de cuenta corriente. Asociada a clientes activos.\n",
        "* Hipotecario: Presencia de cr√©dito hipotecario. Generalmente asociado a mayor compromiso con el banco.\n",
        "* Debito / TC: Indican si el cliente posee tarjetas de d√©bito o cr√©dito, respectivamente.\n",
        "\n",
        "üîπ Actividad Transaccional\n",
        "* Txs_T12: N√∫mero de transacciones en tarjetas de cr√©dito durante el mes m√°s reciente.\n",
        "* CUPO_L1 / L2 / MX: L√≠mites de cr√©dito en compras nacionales, avances y compras internacionales.\n",
        "* FlgAct_T12: Indicador de actividad reciente en tarjeta de cr√©dito.\n",
        "\n",
        "\n",
        "### Problemas Detectados en el An√°lisis Inicial\n",
        "\n",
        "Durante la exploraci√≥n preliminar se identificaron los siguientes desaf√≠os:\n",
        "\n",
        "* Valores nulos en variables clave como Renta, Regi√≥n y Sexo.\n",
        "* Tipos inconsistentes: Algunas variables num√©ricas aparecen como tipo object por uso de comas en lugar de puntos decimales o s√≠mbolos extra√±os.\n",
        "* Outliers: En variables como Txs_T12 y FacAN_T12 se observan valores at√≠picos que podr√≠an afectar el entrenamiento del modelo.\n",
        "\n",
        "Estos problemas se abordar√°n en la etapa de preparaci√≥n de los datos, mediante imputaci√≥n, conversi√≥n de tipos y tratamiento de valores extremos seg√∫n sea necesario.\n",
        "\n",
        "\n",
        "### Exploraciones Iniciales Realizadas\n",
        "\n",
        "Se realizaron algunas visualizaciones para comprender mejor la distribuci√≥n de los datos:\n",
        "\n",
        "* Subsegmentos de Clientes: Permite observar agrupaciones internas seg√∫n nivel de productos, ingresos u otros factores.\n",
        "* Distribuci√≥n Regional: Ofrece perspectiva sobre la presencia geogr√°fica de los clientes.\n",
        "* G√©nero: Ayuda a identificar sesgos o necesidades de segmentaci√≥n.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se carga el conjunto de datos original monopoly y se convierte en un DataFrame de Pandas para facilitar su manipulaci√≥n y an√°lisis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Q6CRZool-rLU"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(monopoly)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNaIaDt4-sJ6"
      },
      "source": [
        "Esta conversi√≥n permite trabajar con una estructura tabular est√°ndar en Python, ideal para aplicar operaciones de limpieza, transformaci√≥n y exploraci√≥n utilizando herramientas del ecosistema cient√≠fico como pandas, numpy y scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzmcNKG9-tdm",
        "outputId": "bfcf660c-192b-4792-c0e5-52ce62a5a74a"
      },
      "outputs": [],
      "source": [
        "#Cantidad de filas(registros) x cantidad de columnas(variables)\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN1B1eDE-vkH"
      },
      "source": [
        "Este comando muestra las dimensiones del DataFrame, es decir:\n",
        "\n",
        "* **Filas** (registros): 51,124 clientes\n",
        "* **Columnas** (variables): 574 caracter√≠sticas asociadas a cada cliente\n",
        "\n",
        "Esta informaci√≥n es clave para dimensionar el volumen de datos con el que se trabajar√° durante las etapas de an√°lisis y modelado.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "iEJpuwcS-0rm",
        "outputId": "85abc02d-30c5-4616-d4cd-9ec707aed1c6"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "cuzyHcuG-5jc",
        "outputId": "f18b749d-9734-45eb-dbab-65fb76aa58f8"
      },
      "outputs": [],
      "source": [
        "# Registro aleatorio\n",
        "df.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "ep1FyzFp-6qq",
        "outputId": "8634ffd5-6732-41e4-9f07-268763e5cd8a"
      },
      "outputs": [],
      "source": [
        "# Genera el gr√°fico de barras\n",
        "plt.figure(figsize=(12, 6))\n",
        "conteo_subsegmento = df['Subsegmento'].value_counts()\n",
        "ax = conteo_subsegmento.plot.bar(rot=45, color='orange', width=0.8)\n",
        "\n",
        "# Etiquetas de texto para cada barra\n",
        "for i, v in enumerate(conteo_subsegmento):\n",
        "    ax.text(i, v + 5, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.xlabel(\"Subsegmento\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.title(\"Distribuci√≥n de Subsegmento\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhZOG7WU-8RD"
      },
      "source": [
        "El gr√°fico de barras muestra que el subsegmento \"170\" es el m√°s frecuente en la base de datos, lo que indica que esta categor√≠a domina en esta variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "ylo-4nj0_Bn9",
        "outputId": "af623645-b177-45e2-b51c-53fbca8532d5"
      },
      "outputs": [],
      "source": [
        "# Genera el gr√°fico de barras\n",
        "plt.figure(figsize=(17, 6))\n",
        "conteo_region = df['Region'].value_counts()\n",
        "ax = conteo_region.plot.bar(rot=0, color='purple', width=0.4)\n",
        "\n",
        "# Etiquetas de texto para cada barra\n",
        "for i, v in enumerate(conteo_region):\n",
        "    ax.text(i, v + 5, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.xlabel(\"Region\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.title(\"Distribuci√≥n de Regi√≥n\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8mFp6z4_DDl"
      },
      "source": [
        "La variable `Region` presenta una fuerte concentraci√≥n en la categor√≠a 13, lo que coincide con su moda. No obstante, se observan inconsistencias en el formato de algunas regiones, como `7` y `7.0`, que representan el mismo valor pero en diferentes tipos. Estas inconsistencias ser√°n tratadas en la fase de limpieza de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "O-nM9LUQ_Ecn",
        "outputId": "5ff3168d-9047-4780-900a-4c9ef34db518"
      },
      "outputs": [],
      "source": [
        "# Genera el gr√°fico de barras\n",
        "plt.figure(figsize=(17, 6))\n",
        "conteo_sexo = df['Sexo'].value_counts()\n",
        "ax = conteo_sexo.plot.bar(rot=0, color='green', width=0.4)\n",
        "\n",
        "# Etiquetas de texto para cada barra\n",
        "for i, v in enumerate(conteo_sexo):\n",
        "    ax.text(i, v + 5, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.xlabel(\"Sexo\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.title(\"Distribuci√≥n de G√©nero: Hombres vs. Mujeres\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MClznn1k_GFB"
      },
      "source": [
        "El gr√°fico de barras muestra que la categor√≠a masculina es la m√°s representada en el conjunto de datos, indicando una mayor proporci√≥n de hombres que mujeres en la muestra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "PjIANmxa_LYB",
        "outputId": "82f76600-d39f-4bf3-aeb4-fa71f89cd722"
      },
      "outputs": [],
      "source": [
        "# Eliminar valores nulos de la columna que se va a graficar\n",
        "data_to_plot = df['Txs_T12'].dropna()\n",
        "\n",
        "# Crear el histograma\n",
        "plt.figure(figsize=(12, 7))\n",
        "plt.hist(data_to_plot, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "\n",
        "# A√±adir t√≠tulo y etiquetas\n",
        "plt.title('Histograma de Transacciones (Txs_T12)', fontsize=16)\n",
        "plt.xlabel('Valor de Transacciones', fontsize=14)\n",
        "plt.ylabel('Frecuencia', fontsize=14)\n",
        "\n",
        "# A√±adir l√≠neas de referencia\n",
        "mean_value = data_to_plot.mean()\n",
        "plt.axvline(mean_value, color='red', linestyle='dashed', linewidth=1, label=f'Media: {mean_value:.2f}')\n",
        "\n",
        "# A√±adir leyenda\n",
        "plt.legend()\n",
        "\n",
        "# A√±adir cuadr√≠cula\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "\n",
        "# Mostrar el histograma\n",
        "plt.tight_layout()  # Ajustar para que todo se muestre correctamente\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI0cFHDRAaFI"
      },
      "source": [
        "El histograma muestra la distribuci√≥n del n√∫mero de transacciones mensuales en tarjeta de cr√©dito (`Txs_T12`). Se observa una alta concentraci√≥n de clientes con pocos movimientos, lo que indica un uso limitado del producto por la mayor√≠a de la base.\n",
        "\n",
        "Una l√≠nea discontinua roja representa la media (**~2.59 transacciones**), visiblemente desplazada hacia la derecha de la mayor√≠a de los datos, lo que evidencia un sesgo fuerte hacia valores bajos.\n",
        "\n",
        "**Observaciones clave**\n",
        "\n",
        "* **Distribuci√≥n sesgada**: La variable presenta una distribuci√≥n fuertemente sesgada a la derecha (right-skewed), con la mayor√≠a de los valores cercanos a cero.\n",
        "* **Media baja**: Aunque el promedio se sit√∫a en 2.59, este valor est√° influido por unos pocos clientes con vol√∫menes transaccionales altos.\n",
        "* **Baja actividad general**: La gran mayor√≠a de los clientes realiza pocas transacciones mensuales, lo que puede implicar baja utilizaci√≥n del producto o inactividad.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "WokBbR2WAq26",
        "outputId": "d3e8abf0-c730-4e4f-8fae-b2b742a435f1"
      },
      "outputs": [],
      "source": [
        "# Filtrar valores nulos en la columna Txs_T12\n",
        "data_to_plot = df['Txs_T12'].dropna()\n",
        "\n",
        "# Crear el diagrama de caja (boxplot)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.boxplot(data_to_plot, vert=False, patch_artist=True,\n",
        "            boxprops=dict(facecolor='lightblue'), whis=10)  # Valor whis ajustado\n",
        "\n",
        "# A√±adir t√≠tulo y etiquetas de los ejes\n",
        "plt.title('Diagrama de Caja de Transacciones (Txs_T12)', fontsize=16)\n",
        "plt.xlabel('Valor de Transacciones', fontsize=14)\n",
        "\n",
        "# Agregar la curva de densidad (KDE plot) con transparencia\n",
        "sns.kdeplot(data_to_plot, color='blue', bw_adjust=0.5, alpha=0.5)  # Ajuste de suavizado y transparencia\n",
        "\n",
        "# Mostrar el diagrama de caja y configurar la cuadr√≠cula\n",
        "plt.grid(axis='x', alpha=0.75)\n",
        "plt.tight_layout()  # Asegurar que todos los elementos se muestren correctamente\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaD7hJaLBAl3"
      },
      "source": [
        "El gr√°fico combina un diagrama de caja con una curva de densidad (KDE) para representar la distribuci√≥n de transacciones mensuales (`Txs_T12`). Ambos elementos revelan una alta concentraci√≥n de valores cercanos a cero, con una ca√≠da abrupta a medida que aumentan.\n",
        "\n",
        "El diagrama de caja muestra una dispersi√≥n estrecha en el rango bajo, con outliers significativos en el extremo derecho, lo que indica la presencia de clientes con actividad inusualmente alta. Los bigotes se extienden hasta aproximadamente 50 transacciones, mientras que el KDE evidencia un fuerte sesgo a la derecha, con un √∫nico pico dominante.\n",
        "\n",
        "**Observaciones clave**\n",
        "\n",
        "* **Alta concentraci√≥n en valores bajos**: La mayor√≠a de los clientes realiza muy pocas transacciones mensuales.\n",
        "* **Outliers relevantes**: Existen casos aislados con vol√∫menes considerablemente m√°s altos que el promedio, lo cual influye en la media.\n",
        "* **Distribuci√≥n asim√©trica**: La forma de la curva y el diagrama de caja evidencian una distribuci√≥n fuertemente sesgada (right-skewed), algo t√≠pico en datos financieros de uso de productos.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOKYnBFnBZ8D"
      },
      "source": [
        "## Preparaci√≥n de los Datos\n",
        "\n",
        "En esta fase, se limpia y transforma el dataset para asegurar que sea consistente, completo y adecuado para el modelado. Dado que se detectaron valores nulos, tipos inconsistentes y outliers, se aplicar√°n transformaciones espec√≠ficas para garantizar la calidad de los datos.\n",
        "\n",
        "\n",
        "### Objetivos de esta Etapa\n",
        "Los principales objetivos de esta fase incluyen:\n",
        "\n",
        "A. **Limpieza de Nulos**: Identificar y tratar valores nulos, tipos incorrectos y registros an√≥malos.\n",
        "\n",
        "B. **Conversi√≥n y Transformaci√≥n**: Asegurar que las variables est√©n correctamente tipadas y representadas.\n",
        "\n",
        "C. **Tratamiento de Outliers**: Limpiar los valores extremos detectados.\n",
        "\n",
        "D. **Normalizaci√≥n/Estandarizaci√≥n**: Escalar variables financieras para asegurar comparabilidad en el modelado.\n",
        "\n",
        "### A. Limpieza de Nulos\n",
        "1. **Variables demogr√°ficas**: Sexo y Regi√≥n contienen valores faltantes. Dado su potencial valor anal√≠tico, se imputar√°n utilizando la moda o el valor m√°s frecuente dentro de subgrupos.\n",
        "2. **Variable financiera ‚Äì Renta**: Dado su impacto en el comportamiento del cliente, se imputar√° usando la mediana por subsegmento, para preservar relaciones contextuales.\n",
        "3. Se incluir√° un heatmap de nulos y un gr√°fico de barras para identificar las columnas m√°s afectadas.\n",
        "\n",
        "### B. Conversi√≥n y Transformaci√≥n\n",
        "1. Variables mal tipadas como object (ej. Renta, CUPO_L1, etc.) ser√°n convertidas a float, eliminando s√≠mbolos o formatos inv√°lidos.\n",
        "2. Regi√≥n ser√° convertida a tipo categ√≥rico, ya que representa una ubicaci√≥n y no una magnitud continua.\n",
        "\n",
        "### C. Tratamiento de Outliers\n",
        "Se identificaron valores extremos en variables como Txs_T12 y FacAN_T12. Para ello:\n",
        "\n",
        "1. Se usar√°n boxplots y curvas KDE para detectar y visualizar estas anomal√≠as.\n",
        "2. El tratamiento aplicar√° reglas basadas en el rango intercuart√≠lico (IQR) o transformaciones logar√≠tmicas, seg√∫n el impacto observado.\n",
        "\n",
        "### D. Normalizaci√≥n y Estandarizaci√≥n\n",
        "1. Estandarizaci√≥n (z-score) se aplicar√° a variables financieras como Renta, CUPO_L1, CUPO_L2 y CUPO_MX.\n",
        "2. Normalizaci√≥n se considerar√° para variables transaccionales si presentan asimetr√≠a fuerte.\n",
        "\n",
        "Estas t√©cnicas aseguran que las variables contribuyan equitativamente en los modelos, sin que su escala domine el aprendizaje.\n",
        "\n",
        "### Preparaci√≥n del Target para el Modelado\n",
        "El target ser√° la suma anual de transacciones con tarjetas (Txs_T01 a Txs_T12), una m√©trica que representa la actividad del cliente durante un a√±o.\n",
        "\n",
        "Pasos:\n",
        "1. **C√°lculo**: Se sumar√°n las columnas mensuales para crear total_transacciones_anual.\n",
        "2. **Transformaci√≥n**: Dado el sesgo hacia valores bajos, se aplicar√° una transformaci√≥n logar√≠tmica (log1p) para mejorar la distribuci√≥n y el ajuste en modelos de regresi√≥n.\n",
        "\n",
        "### Resultado Final\n",
        "Esta etapa deja el dataset limpio, tipado correctamente y escalado, listo para aplicar modelos de regresi√≥n o clasificaci√≥n. La calidad de esta preparaci√≥n es clave para generar modelos robustos y relevantes para las necesidades estrat√©gicas del banco Dormammu.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "qD7q2jTwHYI3"
      },
      "outputs": [],
      "source": [
        "# Selecciona todas las columnas mensuales de transacciones\n",
        "transacciones_mensuales = [f'Txs_T{i:02}' for i in range(1, 13)]\n",
        "\n",
        "# Verifica que las columnas existan en el DataFrame\n",
        "transacciones_mensuales = [col for col in transacciones_mensuales if col in df.columns]\n",
        "\n",
        "# Calcula el total anual de transacciones\n",
        "df['total_transacciones_anual'] = df[transacciones_mensuales].sum(axis=1)\n",
        "target_regresion = df['total_transacciones_anual']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUDq8n2wBbdZ",
        "outputId": "c320d7e5-0302-43c2-b604-9fd23a796c87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             Id  Subsegmento   Sexo   Region     Edad     Renta  Antiguedad  Internauta  Adicional  Dualidad  Monoproducto   Ctacte  Consumo  Hipotecario   Debito  CambioPin  Cuentas       TC  CUPO_L1  CUPO_L2  CUPO_MX\n",
            "count   51124.0      51124.0  51123  51071.0  51124.0   37759.0     51124.0     51124.0    51124.0   51124.0       51124.0  51124.0  51124.0      51124.0  51124.0    31736.0  51124.0  51124.0  51124.0  51124.0  51124.0\n",
            "unique      NaN          NaN      2      NaN      NaN   34276.0         NaN         NaN        NaN       NaN           NaN      NaN      NaN          NaN      NaN        NaN      NaN      NaN      NaN   4591.0   3522.0\n",
            "top         NaN          NaN      H      NaN      NaN  400000.0         NaN         NaN        NaN       NaN           NaN      NaN      NaN          NaN      NaN        NaN      NaN      NaN      NaN      1.0   1000.0\n",
            "freq        NaN          NaN  27410      NaN      NaN     205.0         NaN         NaN        NaN       NaN           NaN      NaN      NaN          NaN      NaN        NaN      NaN      NaN      NaN   5516.0   5058.0\n"
          ]
        }
      ],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "# An√°lisis clientes\n",
        "# Utiliza describe() para obtener estad√≠sticas de variables categ√≥ricas y num√©ricas\n",
        "df_describe = df.iloc[:, 0:21].describe(include='all')\n",
        "\n",
        "# Filtra solo las columnas count, unique, top, freq\n",
        "df_filtered = df_describe.loc[['count', 'unique', 'top', 'freq']]\n",
        "\n",
        "# Muestra solo las estad√≠sticas seleccionadas\n",
        "print(df_filtered)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sy0x9FZfCX--"
      },
      "source": [
        "Se inspeccionaron las estad√≠sticas b√°sicas de un subconjunto de variables utilizando df.describe(include='all'). A continuaci√≥n se destacan observaciones clave por tipo de variable:\n",
        "\n",
        "üîπ Identificadores y Categ√≥ricas\n",
        "* **Id**: Completo (51.124 valores), act√∫a como identificador √∫nico.\n",
        "* **Subsegmento**: Completo, pero no se muestran m√©tricas adicionales al tratarse de una variable categ√≥rica num√©rica.\n",
        "* **Sexo**: Casi completa (1 valor nulo). Dos categor√≠as, con predominio masculino (H, ~27.410 casos).\n",
        "* **Regi√≥n**: Presenta valores faltantes (~53 ausentes). Deber√° convertirse a categ√≥rica y limpiarse por formato inconsistente (ej. \"7\" vs \"7.0\").\n",
        "\n",
        "üîπ Variables Num√©ricas\n",
        "* **Edad, Antig√ºedad**: Completas y num√©ricas continuas, sin issues aparentes.\n",
        "* **Renta**: Incompleta (~26% nulos). Altamente variable (34.276 valores √∫nicos). El valor m√°s frecuente es 400.000 (205 registros), lo que podr√≠a reflejar un default artificial o segmento com√∫n.\n",
        "\n",
        "üîπ Variables Indicadoras (Binarias)\n",
        "* Variables como **Internauta, Adicional, Dualidad, Monoproducto, Ctacte, Consumo, Hipotecario, Debito** est√°n completas y parecen binarias (0/1). Su an√°lisis de frecuencias ser√° √∫til para evaluar activaci√≥n o tenencia de productos.\n",
        "\n",
        "üîπ Variables con Alta Ausencia\n",
        "* **CambioPin**: ~38% de valores nulos. Requiere decisi√≥n sobre imputaci√≥n o eliminaci√≥n, seg√∫n su relevancia en el modelo.\n",
        "\n",
        "üîπ L√≠mites de Cr√©dito\n",
        "* **CUPO_L1 / L2 / MX**: Completos, con alta cardinalidad. Valores frecuentes como 1000.0 o 1.0 podr√≠an corresponder a cupos m√≠nimos estandarizados o defaults del sistema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "Msl8EfQ6Bogh",
        "outputId": "82492834-b7d6-4630-90a3-e39b223bacce"
      },
      "outputs": [],
      "source": [
        "#An√°lisis de movimientos durante el mes de Diciembre\n",
        "df.iloc[::, list(range(21, 67)) + [573]].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "t6yoHc-LB6vr"
      },
      "outputs": [],
      "source": [
        "# Crea una copia del DataFrame para no modificar el original en las siguientes ejecuciones\n",
        "df_copia = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vb2Z4BcDQYj",
        "outputId": "89457676-a08a-43d9-f733-b478b79b2638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Id                int64\n",
            "Subsegmento       int64\n",
            "Sexo             object\n",
            "Region          float64\n",
            "Edad              int64\n",
            "Renta            object\n",
            "Antiguedad        int64\n",
            "Internauta        int64\n",
            "Adicional         int64\n",
            "Dualidad          int64\n",
            "Monoproducto      int64\n",
            "Ctacte            int64\n",
            "Consumo           int64\n",
            "Hipotecario       int64\n",
            "Debito            int64\n",
            "CambioPin       float64\n",
            "Cuentas           int64\n",
            "TC                int64\n",
            "CUPO_L1           int64\n",
            "CUPO_L2          object\n",
            "CUPO_MX          object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Mostrar la media\n",
        "print(df.iloc[:, 0:21].dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbO0mHA1DcgY"
      },
      "source": [
        "Se inspeccionaron los tipos de las primeras columnas del dataset para identificar inconsistencias relevantes:\n",
        "\n",
        "* **Categ√≥ricas mal tipadas**: Variables como Renta, CUPO_L2, CUPO_MX est√°n en formato object, lo que sugiere problemas de formato (p. ej., uso de comas decimales o s√≠mbolos no num√©ricos). Se transformar√°n a float tras limpieza.\n",
        "* **Variables binarias**: Columnas como Internauta, Ctacte, Hipotecario, entre otras, est√°n correctamente como int, aunque podr√≠an representarse como bool para mejorar la sem√°ntica.\n",
        "* **Regi√≥n**: Est√° en formato float64, pero representa una categor√≠a geogr√°fica. Ser√° convertida a tipo category para reflejar su naturaleza.\n",
        "\n",
        "Este diagn√≥stico guiar√° la conversi√≥n y estandarizaci√≥n de tipos durante la limpieza.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHtzUJQeDxkG",
        "outputId": "9c240cdc-5c8f-4941-f838-8b6c26e19c0a"
      },
      "outputs": [],
      "source": [
        "# Seleccionar solo las columnas num√©ricas\n",
        "df_numerico = df.iloc[:, 0:21].select_dtypes(include=[np.number])\n",
        "\n",
        "# Calcular la mediana de las columnas num√©ricas\n",
        "mediana = df_numerico.median()\n",
        "\n",
        "# Mostrar la mediana\n",
        "print(mediana)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2EpAAFREXZR",
        "outputId": "1693fc4c-7931-463f-9d42-daf0db62c172"
      },
      "outputs": [],
      "source": [
        "subset_df = df.iloc[:, :21]\n",
        "moda_result = subset_df.mode()\n",
        "print(moda_result.iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJnyBRG3Eg1i"
      },
      "source": [
        "Se analizaron las medianas y modas de las variables clave para identificar tendencias centrales en el comportamiento y perfil de los clientes:\n",
        "\n",
        "üîπPerfil Demogr√°fico y General\n",
        "* **Edad**: Mediana de 35 a√±os, con moda en 27 ‚Üí clientela relativamente joven.\n",
        "* **Antig√ºedad**: Mediana de 25 meses, moda de 10 meses ‚Üí predominan clientes nuevos.\n",
        "* **Regi√≥n**: Moda en 13.0, lo que indica alta concentraci√≥n geogr√°fica en esa zona (probablemente Santiago).\n",
        "* **Sexo**: Moda ‚ÄúH‚Äù ‚Üí mayor√≠a de los clientes son hombres.\n",
        "\n",
        "üîπProductos y Servicios\n",
        "* **Internauta, D√©bito, Ctacte, CambioPin**: Moda de 1, lo que sugiere alto uso de servicios digitales y productos b√°sicos.\n",
        "* **Cr√©ditos (Consumo, Hipotecario), Adicionales, Dualidad**: Moda de 0 ‚Üí baja adopci√≥n de productos avanzados.\n",
        "\n",
        "üîπIngresos y Cupos\n",
        "* **Renta**:\n",
        "  * Mediana no mostrada por estar incompleta (~26% de nulos).\n",
        "  * Moda en 400.000 ‚Üí posible valor com√∫n o default.\n",
        "* **CUPO_L1**: Mediana en 800.000, moda en 200.000 ‚Üí l√≠mites estandarizados.\n",
        "* **CUPO_MX**: Moda en 1.000, indicando cupos internacionales bajos.\n",
        "\n",
        "üîπTarjetas de Cr√©dito\n",
        "* **TC (n√∫mero de tarjetas)**: Mediana de 2, moda de 1 ‚Üí mayor√≠a con una sola tarjeta, pero con una fracci√≥n relevante que posee m√°s.\n",
        "\n",
        "üîπConclusi√≥n\n",
        "\n",
        "El banco mantiene una base joven, mayoritariamente masculina y reciente, con acceso preferente a productos digitales y b√°sicos. La baja adopci√≥n de productos financieros complejos sugiere oportunidades en venta cruzada (cross-sell) o estrategias de fidelizaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "lwfI19RwE-Fd",
        "outputId": "074e70ad-9eb3-4136-dc86-1db5aec22344"
      },
      "outputs": [],
      "source": [
        "df_copia.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcLwJfVeFJvB"
      },
      "source": [
        "Se identificaron valores nulos en un subconjunto de variables:\n",
        "\n",
        "| Variable | Nulos | Comentario                                                               |\n",
        "| -------- | ----- | ------------------------------------------------------------------------ |\n",
        "| Sexo     | 1     | Valor faltante puntual, puede imputarse con la moda                      |\n",
        "| Region   | 53    | Faltantes m√°s relevantes; requiere an√°lisis antes de imputar o eliminar. |\n",
        "| Otras    | 0     | Completas                                                                |\n",
        "\n",
        "üîπObservaciones clave:\n",
        "* La mayor√≠a de las columnas cr√≠ticas est√°n completas.\n",
        "* Las columnas con nulos tienen baja proporci√≥n sobre el total (51.124 registros), por lo que imputar con valores dominantes o eliminar esos pocos registros no tendr√≠a impacto significativo.\n",
        "* En etapas posteriores se definir√° si se imputan (Region, Sexo) o si se excluyen, seg√∫n su relevancia en el modelado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdUQZcX5P9g6",
        "outputId": "dd2d9852-ec0e-475a-c2f1-c07246f98ca6"
      },
      "outputs": [],
      "source": [
        "# Conversi√≥n de Renta a num√©rico\n",
        "df_copia['Renta'] = pd.to_numeric(df['Renta'], errors='coerce')\n",
        "\n",
        "# Imputaciones\n",
        "df_copia['Sexo'] = df_copia['Sexo'].fillna(df['Sexo'].mode()[0])\n",
        "df_copia['Region'] = df_copia['Region'].fillna(df['Region'].mode()[0])\n",
        "df_copia['Renta'] = df_copia.groupby('Subsegmento')['Renta'].transform(lambda x: x.fillna(x.median()))\n",
        "df_copia['CambioPin'] = df_copia['CambioPin'].fillna(df['CambioPin'].median())\n",
        "\n",
        "# Nota: Aunque actualmente no hay nulos en 'Renta' ni 'CambioPin',\n",
        "# estas imputaciones est√°n incluidas como medida preventiva para garantizar\n",
        "# estabilidad si se aplicara el pipeline a nuevos datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZSyASLqQj0g"
      },
      "source": [
        "Este bloque de c√≥digo ejecuta la limpieza e imputaci√≥n de datos faltantes en el DataFrame df_copia, con el objetivo de garantizar que las columnas cr√≠ticas est√©n completas y correctamente tipadas. Esta etapa es esencial para evitar errores en etapas posteriores del an√°lisis y asegurar la integridad del pipeline de modelado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T4vXRt5QBWQ",
        "outputId": "7528e0b7-6cf1-4a18-bfe3-3de8ffc52f99"
      },
      "outputs": [],
      "source": [
        "# Convertir variables num√©ricas que est√°n en formato object a float\n",
        "# Primero, eliminar caracteres especiales que impidan la conversi√≥n\n",
        "\n",
        "# Remover posibles s√≠mbolos de moneda y convertir a num√©rico\n",
        "df_copia['Renta'] = pd.to_numeric(df['Renta'], errors='coerce')\n",
        "df_copia['CUPO_L1'] = pd.to_numeric(df['CUPO_L1'], errors='coerce')\n",
        "df_copia['CUPO_L2'] = pd.to_numeric(df['CUPO_L2'], errors='coerce')\n",
        "df_copia['CUPO_MX'] = pd.to_numeric(df['CUPO_MX'], errors='coerce')\n",
        "\n",
        "# Verificar la conversi√≥n\n",
        "print(df[['Renta', 'CUPO_L1', 'CUPO_L2', 'CUPO_MX']].dtypes)\n",
        "\n",
        "# Convertir `Region` a tipo categ√≥rico\n",
        "df_copia['Region'] = df_copia['Region'].astype('category')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4Dp6HdYYQCXK"
      },
      "outputs": [],
      "source": [
        "# Funci√≥n para filtrar outliers utilizando el rango intercuart√≠lico (IQR)\n",
        "def remove_outliers(df, column):\n",
        "    Q1 = df_copia[column].quantile(0.25)\n",
        "    Q3 = df_copia[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    # Filtra los datos fuera del rango IQR\n",
        "    filtered_df = df_copia[(df_copia[column] >= Q1 - 1.5 * IQR) & (df_copia[column] <= Q3 + 1.5 * IQR)]\n",
        "    return filtered_df\n",
        "\n",
        "# Aplicar la funci√≥n en algunas de las variables financieras clave\n",
        "df_copia = remove_outliers(df_copia, 'Renta')\n",
        "df_copia = remove_outliers(df_copia, 'CUPO_L1')\n",
        "df_copia = remove_outliers(df_copia, 'CUPO_L2')\n",
        "df_copia = remove_outliers(df_copia, 'CUPO_MX')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO1NH22QQDWd",
        "outputId": "620e15b6-8679-40a4-f938-a44f5345aa79"
      },
      "outputs": [],
      "source": [
        "# Estandarizaci√≥n de variables financieras clave (para modelos de regresi√≥n lineal o log√≠stica)\n",
        "scaler = StandardScaler()\n",
        "df_copia[['Renta', 'CUPO_L1', 'CUPO_L2', 'CUPO_MX']] = scaler.fit_transform(df_copia[['Renta', 'CUPO_L1', 'CUPO_L2', 'CUPO_MX']])\n",
        "\n",
        "# Normalizaci√≥n de las variables de transacciones (para modelos como K-Means)\n",
        "transacciones_mensuales = [f'Txs_T{i:02}' for i in range(1, 13)]\n",
        "scaler = MinMaxScaler()\n",
        "df_copia[transacciones_mensuales] = scaler.fit_transform(df_copia[transacciones_mensuales])\n",
        "\n",
        "# Revisar los resultados de la normalizaci√≥n y estandarizaci√≥n\n",
        "print(df_copia[['Renta', 'CUPO_L1', 'CUPO_L2', 'CUPO_MX']].describe())\n",
        "print(df_copia[transacciones_mensuales].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "ARQTRAhlNYf-",
        "outputId": "70d9bdc1-e32a-4247-84a1-0ed5f71ffa22"
      },
      "outputs": [],
      "source": [
        "# Verificar el tipo de dato del target\n",
        "print(\"Tipo de dato del target:\", df_copia['total_transacciones_anual'].dtype)\n",
        "\n",
        "# Visualizar la distribuci√≥n del target con un histograma y un gr√°fico de densidad\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Histograma\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df_copia['total_transacciones_anual'].dropna(), bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribuci√≥n del Target (Histograma)')\n",
        "plt.xlabel('Total de Transacciones Anual')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "# Gr√°fico de Densidad (KDE)\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.kdeplot(df_copia['total_transacciones_anual'].dropna(), color='blue')\n",
        "plt.title('Distribuci√≥n del Target (Densidad)')\n",
        "plt.xlabel('Total de Transacciones Anual')\n",
        "plt.ylabel('Densidad')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HvWfABMNtly"
      },
      "source": [
        "üîπAn√°lisis del Target\n",
        "* **Tipo de Dato**:\n",
        "  * El target, correspondiente al total anual de transacciones por cliente, est√° en formato float64. Este tipo de dato es adecuado para modelado de regresi√≥n, ya que representa una variable num√©rica continua.\n",
        "* **Distribuci√≥n del Target ‚Äì Histograma**:\n",
        "  * La mayor√≠a de los clientes concentra sus transacciones anuales en valores bajos, pr√≥ximos a cero.\n",
        "  * La distribuci√≥n presenta un sesgo positivo (cola a la derecha), con una frecuencia decreciente a medida que aumentan las transacciones. Pocos clientes superan las 100 transacciones anuales.\n",
        "* **Distribuci√≥n del Target ‚Äì Curva de Densidad (KDE)**:\n",
        "  * El KDE reafirma la concentraci√≥n en valores bajos, con un pico marcado en la zona inicial de la escala y una larga cola hacia la derecha.\n",
        "  * Este patr√≥n sugiere una fuerte asimetr√≠a en la base de clientes: una mayor√≠a con baja actividad y una minor√≠a altamente activa.\n",
        "\n",
        "üîπConclusiones\n",
        "* **Distribuci√≥n Sesgada**:\n",
        "  * El sesgo del target puede afectar la capacidad de los modelos de regresi√≥n para generalizar correctamente, especialmente si el objetivo es predecir con precisi√≥n los valores m√°s altos (clientes altamente activos).\n",
        "* **Transformaci√≥n Recomendada**:\n",
        "  * Aplicar una transformaci√≥n logar√≠tmica o ra√≠z cuadrada podr√≠a reducir la asimetr√≠a y estabilizar la varianza, facilitando un aprendizaje m√°s equilibrado por parte del modelo. Esta decisi√≥n depender√° de si se prioriza mejorar el ajuste general o capturar mejor los valores extremos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "594m5Ue0N4jb",
        "outputId": "8d2b2d0e-4028-4e4a-8e44-5470a4bf7d05"
      },
      "outputs": [],
      "source": [
        "# Aplicar una transformaci√≥n logar√≠tmica al target para reducir el sesgo\n",
        "df_copia['total_transacciones_anual_log'] = np.log1p(df_copia['total_transacciones_anual'])\n",
        "\n",
        "# Visualizar la distribuci√≥n del target transformado\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df_copia['total_transacciones_anual_log'].dropna(), bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribuci√≥n del Target Transformado (Histograma)')\n",
        "plt.xlabel('Log(1 + Total de Transacciones Anual)')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.kdeplot(df_copia['total_transacciones_anual_log'].dropna(), color='blue')\n",
        "plt.title('Distribuci√≥n del Target Transformado (Densidad)')\n",
        "plt.xlabel('Log(1 + Total de Transacciones Anual)')\n",
        "plt.ylabel('Densidad')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avie3c_OOfb5"
      },
      "source": [
        "üîπEvaluaci√≥n de la Transformaci√≥n Logar√≠tmica del Target\n",
        "\n",
        "La aplicaci√≥n de una transformaci√≥n logar√≠tmica al target ha generado mejoras significativas en su distribuci√≥n, lo que tiene implicancias directas sobre la calidad del modelado.\n",
        "* **Distribuci√≥n M√°s Sim√©trica**:\n",
        "  * Tras la transformaci√≥n, los valores del target se distribuyen de forma m√°s equilibrada alrededor del centro. La forma de la curva se aproxima a una distribuci√≥n normal, lo que favorece el uso de modelos que asumen simetr√≠a en los residuos.\n",
        "* **Reducci√≥n del Sesgo**:\n",
        "  * El sesgo positivo presente en los datos originales ‚Äîconcentraci√≥n en valores bajos y una cola larga hacia la derecha‚Äî se ha mitigado. Esto reduce la influencia desproporcionada de los valores extremos sobre el ajuste del modelo.\n",
        "* **Beneficios para el Modelado Predictivo**:\n",
        "  * Una distribuci√≥n m√°s balanceada permite que los algoritmos de regresi√≥n capturen relaciones con mayor precisi√≥n a lo largo de todo el rango del target. Esto puede traducirse en:\n",
        "    * Mejor capacidad de generalizaci√≥n.\n",
        "    * Reducci√≥n del error en predicciones extremas.\n",
        "    * Menor sensibilidad a outliers.\n",
        "\n",
        "üîπJustificaci√≥n del Uso de np.log1p para la Transformaci√≥n del Target\n",
        "\n",
        "Se utiliz√≥ np.log1p(x) en lugar de np.log(x) para transformar el target debido a las siguientes razones:\n",
        "* **Manejo de ceros sin errores**:\n",
        "  * A diferencia de np.log(x), que no admite valores iguales a cero (lo que generar√≠a un error o -inf), np.log1p(x) calcula log(1 + x), permitiendo transformar valores cero de forma segura. Esto es especialmente relevante en contextos donde el target puede representar conteos o montos que incluyen ceros.\n",
        "* **Estabilidad num√©rica**:\n",
        "  * np.log1p ofrece mayor precisi√≥n para valores peque√±os de x, evitando errores de redondeo o p√©rdida de informaci√≥n significativa que puede ocurrir con np.log(1 + x) de forma manual.\n",
        "* **Conservaci√≥n de la escala relativa**:\n",
        "  * Aunque suaviza menos que una logaritmizaci√≥n m√°s agresiva, np.log1p mantiene relaciones proporcionales importantes para modelos de regresi√≥n que requieren interpretar variaciones relativas.\n",
        "* **Conclusi√≥n**:\n",
        "  * El uso de np.log1p garantiza una transformaci√≥n segura, estable y efectiva del target, facilitando una mejor distribuci√≥n sin comprometer la integridad de los datos.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BgWYI1KG3dp"
      },
      "source": [
        "## Modelado\n",
        "\n",
        "En esta etapa se implementan modelos de machine learning con el objetivo de identificar patrones en el comportamiento de los clientes y predecir su probabilidad de retenci√≥n.\n",
        "\n",
        "üîπObjetivo del Modelado\n",
        "\n",
        "El prop√≥sito principal es construir un modelo supervisado capaz de clasificar a los clientes en funci√≥n de su comportamiento financiero, espec√≠ficamente en relaci√≥n con su nivel de actividad transaccional anual (target binarizado). Este tipo de an√°lisis es clave para anticipar abandono o desuso de productos financieros y orientar campa√±as de retenci√≥n.\n",
        "\n",
        "üîπModelos Considerados\n",
        "\n",
        "* **Regresi√≥n Log√≠stica**:\n",
        "  * Modelo base de clasificaci√≥n binaria, √∫til por su interpretabilidad y capacidad para manejar relaciones lineales entre variables independientes y la probabilidad de una clase (en este caso, clientes con alta o baja actividad).\n",
        "* **√Årboles de Decisi√≥n**:\n",
        "  * Utilizados como alternativa m√°s flexible, permiten capturar relaciones no lineales y generan reglas claras para interpretar la segmentaci√≥n de clientes. Son especialmente √∫tiles en entornos con m√∫ltiples variables categ√≥ricas y umbrales de decisi√≥n.\n",
        "* **K-Means (Clustering)**:\n",
        "  * Aplicado como m√©todo no supervisado para segmentar clientes en grupos seg√∫n similitudes en sus patrones de uso. Este an√°lisis complementa el modelado supervisado, ya que permite descubrir perfiles ocultos en la base de clientes, incluso sin etiquetas expl√≠citas.\n",
        "\n",
        "üîπPreparaci√≥n de los Datos para el Modelado\n",
        "* **Divisi√≥n de Datos**:\n",
        "  * El dataset se divide en un conjunto de entrenamiento (80%) y uno de prueba (20%) mediante train_test_split, garantizando que la evaluaci√≥n del modelo se realice sobre datos no vistos.\n",
        "* **Validaci√≥n Cruzada**:\n",
        "  * En el caso de modelos supervisados como la regresi√≥n log√≠stica, se emplea validaci√≥n cruzada (k-fold) para evaluar la estabilidad del modelo y prevenir el sobreajuste. Esto permite obtener m√©tricas m√°s robustas en escenarios con alta varianza.\n",
        "* **Estandarizaci√≥n y Codificaci√≥n**:\n",
        "  * Se construye un pipeline de preprocesamiento que transforma las variables num√©ricas (escaladas con StandardScaler) y categ√≥ricas (codificadas con OneHotEncoder), permitiendo que el modelo procese adecuadamente las distintas escalas y tipos de variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPt8gU06IiXZ"
      },
      "outputs": [],
      "source": [
        "# Selecci√≥n de features y target (con transformaci√≥n logar√≠tmica)\n",
        "X = df_copia.drop(columns=['total_transacciones_anual', 'total_transacciones_anual_log'])  # todas las columnas menos el target\n",
        "y = df_copia['total_transacciones_anual_log']  # el target log-transformado\n",
        "\n",
        "# Divisi√≥n de los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BLFxj1Koune"
      },
      "source": [
        "üîπDivisi√≥n de Variables y Separaci√≥n de Datos\n",
        "\n",
        "1. **Selecci√≥n de Variables Predictoras (X) y Target (y)**\n",
        "* Se define `X` como el subconjunto de variables independientes, excluyendo tanto el target original (`total_transacciones_anual`) como su transformaci√≥n (`total_transacciones_anual_log`), para evitar fuga de informaci√≥n durante el entrenamiento.\n",
        "* La variable objetivo y corresponde a `total_transacciones_anual_log`, una transformaci√≥n logar√≠tmica del total de transacciones anuales. Esta transformaci√≥n permite suavizar la distribuci√≥n del target y mejora el rendimiento en modelos sensibles al sesgo como la regresi√≥n lineal.\n",
        "\n",
        "2. **Divisi√≥n del Dataset en Entrenamiento y Prueba**\n",
        "* Se utiliza `train_test_split` para dividir el dataset en un conjunto de entrenamiento (`80%`) y otro de prueba (`20%`).\n",
        "* El par√°metro `random_state=42` garantiza la reproducibilidad del experimento, asegurando que la partici√≥n de datos sea consistente a lo largo de ejecuciones sucesivas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEaf_Y0NNUf5",
        "outputId": "f94c6f37-7beb-49ff-80dd-59b0cead2d91"
      },
      "outputs": [],
      "source": [
        "# Identificar columnas que son de tipo `object`\n",
        "object_columns = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Intentar convertir columnas num√©ricas con formato de coma decimal\n",
        "for col in object_columns:\n",
        "    try:\n",
        "        # Reemplazar comas con puntos y convertir a float\n",
        "        X[col] = X[col].str.replace(',', '.').astype(float)\n",
        "        print(f\"Columna {col} convertida a num√©rica con √©xito.\")\n",
        "    except ValueError:\n",
        "        print(f\" /!\\ Advertencia: Columna '{col}' no se pudo convertir a num√©rica, a√∫n contiene valores no num√©ricos.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npNpkKv5o_km"
      },
      "source": [
        "üîπConversi√≥n de Columnas Categ√≥ricas Mal Tipadas a Formato Num√©rico\n",
        "\n",
        "Este bloque de c√≥digo estandariza columnas originalmente clasificadas como object que, en realidad, contienen datos num√©ricos representados con comas decimales (formato com√∫n en regiones hispanohablantes).\n",
        "\n",
        "* **Detecci√≥n de Columnas Problem√°ticas**\n",
        "  * Se identifican todas las columnas con tipo de dato `object`, ya que estas pueden contener valores num√©ricos mal interpretados como texto por el parser inicial (por ejemplo, `982,45`).\n",
        "* **Normalizaci√≥n del Formato Num√©rico**\n",
        "  * Para cada columna identificada, se reemplazan las comas por puntos (',' ‚Üí '.') y se intenta convertir los valores a tipo `float`.\n",
        "  * Las conversiones exitosas se notifican con un mensaje informativo; si la conversi√≥n falla (por ejemplo, por contener texto irreconciliable con una representaci√≥n num√©rica), se genera una advertencia controlada.\n",
        "\n",
        "Este paso es fundamental para evitar errores durante el preprocesamiento y asegurar que todas las variables num√©ricas est√©n en el formato adecuado para el an√°lisis y modelado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8Fswv68LxZ4",
        "outputId": "eeea02f3-0a96-4370-b632-286b8fb3fee8"
      },
      "outputs": [],
      "source": [
        "# Detectar columnas categ√≥ricas en el DataFrame X\n",
        "categorical_columns = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Identificar qu√© columnas pueden utilizar LabelEncoder y cu√°les requieren OneHotEncoder\n",
        "label_encode_cols = []\n",
        "onehot_encode_cols = []\n",
        "\n",
        "# Iterar sobre las columnas categ√≥ricas y contar las categor√≠as √∫nicas\n",
        "for col in categorical_columns:\n",
        "    unique_values = X[col].nunique()\n",
        "\n",
        "    if unique_values == 2:\n",
        "        # Si tiene solo dos categor√≠as, se puede usar LabelEncoder\n",
        "        label_encode_cols.append(col)\n",
        "    else:\n",
        "        # Si tiene m√°s de dos categor√≠as, se recomienda OneHotEncoder\n",
        "        onehot_encode_cols.append(col)\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"Columnas para LabelEncoder (2 categor√≠as):\", label_encode_cols)\n",
        "print(\"Columnas para OneHotEncoder (m√°s de 2 categor√≠as):\", onehot_encode_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNbqhEzQpVG3"
      },
      "source": [
        "üîπClasificaci√≥n de Variables Categ√≥ricas para Codificaci√≥n\n",
        "\n",
        "Este bloque organiza las columnas categ√≥ricas en funci√≥n del tipo de codificaci√≥n m√°s apropiado, optimizando as√≠ su tratamiento antes del modelado.\n",
        "\n",
        "* **Identificaci√≥n de Columnas Categ√≥ricas**\n",
        "  * Se seleccionan todas las columnas en `X` con tipo `object` o `category`, lo que usualmente representa variables con etiquetas no num√©ricas (por ejemplo, g√©nero, regi√≥n, tipo de producto).\n",
        "* **Segmentaci√≥n seg√∫n N√∫mero de Categor√≠as**\n",
        "  * Cada columna categ√≥rica se analiza seg√∫n su cardinalidad (cantidad de valores √∫nicos).\n",
        "    * Si tiene exactamente dos categor√≠as, se asigna a `label_encode_cols`, ya que puede representarse eficientemente con `LabelEncoder`.\n",
        "    * Si tiene m√°s de dos categor√≠as, se asigna a `onehot_encode_cols`, ya que `OneHotEncoder` evita introducir orden impl√≠cito en variables no ordinales.\n",
        "* **Resultado del An√°lisis**\n",
        "  * Se imprime una lista diferenciada de columnas seg√∫n el tipo de codificaci√≥n recomendado. Esto facilita el dise√±o posterior del pipeline de preprocesamiento, asegurando un tratamiento correcto de las variables categ√≥ricas en el modelo.\n",
        "\n",
        "Este paso es crucial para garantizar que el modelo reciba entradas num√©ricas bien estructuradas, evitando sesgos impl√≠citos y mejorando su rendimiento general."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk1c4U-wNeEY"
      },
      "outputs": [],
      "source": [
        "# Aplicar LabelEncoder a las columnas binarias\n",
        "for col in label_encode_cols:\n",
        "    X[col] = LabelEncoder().fit_transform(X[col])\n",
        "\n",
        "# Aplicar OneHotEncoder a las columnas con m√°s de 2 categor√≠as\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=True)\n",
        "X_encoded = encoder.fit_transform(X[onehot_encode_cols])\n",
        "\n",
        "# Combinar las caracter√≠sticas num√©ricas y categ√≥ricas codificadas\n",
        "X_rest = X.drop(columns=onehot_encode_cols)\n",
        "X_sparse = hstack([X_rest, X_encoded])\n",
        "\n",
        "# Divisi√≥n en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sparse, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcPPIl4ppgAZ"
      },
      "source": [
        "üîπCodificaci√≥n de Variables Categ√≥ricas y Preparaci√≥n Final de Datos\n",
        "\n",
        "* **Codificaci√≥n con LabelEncoder**\n",
        "  * Se aplica a columnas binarias (`label_encode_cols`) para transformarlas directamente en valores num√©ricos (`0` y `1`), manteniendo la estructura original sin expansi√≥n dimensional.\n",
        "* **Codificaci√≥n con OneHotEncoder**\n",
        "  * Para columnas con m√°s de dos categor√≠as (`onehot_encode_cols`), se utiliza `OneHotEncoder` con `drop='first'` para evitar multicolinealidad.\n",
        "  * La opci√≥n `sparse_output=True` genera una matriz dispersa eficiente en memoria.\n",
        "* **Integraci√≥n del Dataset Codificado**\n",
        "  * `X_rest` conserva las columnas no categ√≥ricas.\n",
        "  * La matriz final `X_sparse` se obtiene al combinar `X_rest` con la matriz codificada `X_encoded` mediante `hstack`, integrando variables categ√≥ricas y num√©ricas en una estructura apta para el modelado.\n",
        "* **Divisi√≥n de Datos**\n",
        " * Los datos se dividen en conjuntos de entrenamiento y prueba (`X_train`, `X_test`, `y_train`, `y_test`) mediante `train_test_split`, reservando el 20% para validaci√≥n (`test_size=0.2`) y asegurando reproducibilidad con `random_state=42`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V9SIlB1Ofdd"
      },
      "outputs": [],
      "source": [
        "# Imputaci√≥n de NaN con la m√°s frecuente de cada columna\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "X_train = imputer.fit_transform(X_train)\n",
        "X_test = imputer.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Axa1HfCp19D"
      },
      "source": [
        "üîπImputaci√≥n de Valores Faltantes con SimpleImputer\n",
        "* **Definici√≥n del Imputador**\n",
        "  * Se utiliza `SimpleImputer(strategy=\"most_frequent\")` para reemplazar los valores nulos (`NaN`) por el valor m√°s frecuente de cada columna. Esta estrategia es √∫til para variables categ√≥ricas o num√©ricas discretas con una moda dominante.\n",
        "* **Aplicaci√≥n en el Conjunto de Entrenamiento**\n",
        "  * `fit_transform(X_train)` ajusta el imputador aprendiendo el valor m√°s frecuente de cada columna y lo aplica inmediatamente sobre `X_train`, garantizando consistencia interna.\n",
        "* **Aplicaci√≥n en el Conjunto de Prueba**\n",
        "  * `transform(X_test)` aplica la imputaci√≥n aprendida desde `X_train` a `X_test`, asegurando que no se introduzca informaci√≥n del conjunto de prueba durante el entrenamiento (principio de no-leakage)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZLUQWnJKj23",
        "outputId": "52a06b5c-18d1-42d3-d097-c499dcc89b36"
      },
      "outputs": [],
      "source": [
        "# Entrenar un modelo de Regresi√≥n Lineal\n",
        "linear_model = LinearRegression()\n",
        "\n",
        "# Realizar predicciones con validaci√≥n cruzada\n",
        "y_pred_cv = cross_val_predict(linear_model, X_train, y_train, cv=4)\n",
        "\n",
        "# Calcular las m√©tricas en validaci√≥n cruzada\n",
        "mse_cv = mean_squared_error(y_train, y_pred_cv)\n",
        "mae_cv = mean_absolute_error(y_train, y_pred_cv)\n",
        "rmse_cv = np.sqrt(mse_cv)\n",
        "\n",
        "print(f\"MSE Regresi√≥n Lineal: {mse_cv:.4f}\")\n",
        "print(f\"MAE Regresi√≥n Lineal: {mae_cv:.4f}\")\n",
        "print(f\"RMSE Regresi√≥n Lineal: {rmse_cv:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPITPpu8qG3W"
      },
      "source": [
        "üîπEvaluaci√≥n del Modelo de Regresi√≥n Lineal (Baseline)\n",
        "\n",
        "Este modelo fue utilizado √∫nicamente como baseline inicial para establecer un punto de comparaci√≥n. No se considera adecuado para producci√≥n debido a limitaciones en su rendimiento predictivo.\n",
        "\n",
        "1. **Entrenamiento y Evaluaci√≥n**\n",
        "* **Modelo**: `LinearRegression()`\n",
        "* **Entrenamiento**: Se ajust√≥ sobre los datos `X_train` y `y_train`.\n",
        "* **Validaci√≥n cruzada**: Se utiliz√≥ cross_val_score con 4 pliegues (`cv=4`), evaluando con la m√©trica neg_mean_squared_error.\n",
        "* **Promedio del MSE**: Se calcul√≥ invirtiendo el signo de la media para interpretar el error en forma positiva.\n",
        "\n",
        "2. **Resultados**\n",
        "* **MSE (Mean Squared Error)**: ~0.31\n",
        "  * Refleja un error cuadr√°tico promedio bajo en la escala logar√≠tmica del target (log(1 + transacciones)), que va aproximadamente de 0 a 6.\n",
        "* **MAE (Mean Absolute Error)**: ~0.43\n",
        "  * Indica que la desviaci√≥n promedio de las predicciones es de 0.43 unidades en la escala logar√≠tmica.\n",
        "* **RMSE (Root Mean Squared Error)**: ~0.55\n",
        "  * Proporciona una m√©trica m√°s intuitiva de error promedio. En este caso, tambi√©n es bajo, lo que implica que no hay errores extremos frecuentes.\n",
        "\n",
        "3. **Conclusi√≥n**\n",
        "Aunque los errores obtenidos son bajos en t√©rminos absolutos, la regresi√≥n lineal no captura relaciones no lineales ni interacciones complejas entre variables. Por ello, aunque sirve como referencia inicial, se requiere un modelo m√°s robusto para mejorar la capacidad predictiva y explotar mejor la estructura del dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOBewSsYv5F-",
        "outputId": "736ff131-0843-4a8a-aec6-7a931ff93309"
      },
      "outputs": [],
      "source": [
        "# Paso 1: Convertir variables categ√≥ricas en X a num√©ricas usando OneHotEncoder\n",
        "X_encoded = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Paso 2: Concatenar X_encoded e y en un √∫nico DataFrame para el c√°lculo de la correlaci√≥n\n",
        "if 'target' in X_encoded.columns:\n",
        "    X_encoded = X_encoded.drop(columns='target')\n",
        "df_combined = pd.concat([X_encoded, y.rename(\"target\")], axis=1)\n",
        "\n",
        "# Paso 3: Calcular la matriz de correlaci√≥n en el DataFrame combinado\n",
        "correlation_matrix = df_combined.corr()\n",
        "\n",
        "# Paso 4: Extraer la fila de correlaci√≥n del target y ordenarla usando 'by'\n",
        "try:\n",
        "    # Filtramos las correlaciones con respecto al 'target', convirti√©ndolas en un DataFrame si es necesario\n",
        "    target_correlation = correlation_matrix[['target']].drop(index='target').sort_values(by='target', ascending=False)\n",
        "\n",
        "    # Mostrar las 10 caracter√≠sticas con mayor correlaci√≥n con el target\n",
        "    print(\"Top 10 features m√°s correlacionados con el target:\")\n",
        "    print(target_correlation.head(10))\n",
        "except KeyError:\n",
        "    print(\"Error: La columna 'target' no se encuentra en correlation_matrix.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KdfGIBtu46l"
      },
      "source": [
        "üîπSelecci√≥n del Target para Modelos de Clasificaci√≥n\n",
        "\n",
        "* **Target elegido**: Probabilidad de Retenci√≥n de Clientes\n",
        "  * **Definici√≥n**: Clasificaci√≥n binaria que distingue entre clientes con alta probabilidad de abandono y aquellos con baja probabilidad de abandono.\n",
        "    *  Esta variable se puede construir a partir de indicadores como frecuencia de transacciones, antig√ºedad, uso de productos financieros y niveles de actividad reciente.\n",
        "  * **Justificaci√≥n de Negocio**\n",
        "    * Este target es altamente accionable. Permite a Dormammu identificar clientes en riesgo de fuga y dise√±ar estrategias de retenci√≥n proactiva. Al anticipar la p√©rdida de clientes, el banco puede intervenir con ofertas personalizadas, campa√±as de fidelizaci√≥n o ajustes en productos, mejorando la rentabilidad a largo plazo y reduciendo los costos de adquisici√≥n de nuevos clientes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "k45_qqKs3FF8"
      },
      "outputs": [],
      "source": [
        "# Filtrar solo columnas num√©ricas para escalamiento\n",
        "num_features = X.select_dtypes(include=[np.number]).columns\n",
        "X[num_features] = StandardScaler().fit_transform(X[num_features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrNvoCZOgNQB",
        "outputId": "2f20b77d-2766-4e11-fe2a-88da4c72825e"
      },
      "outputs": [],
      "source": [
        "# Codificaci√≥n One-Hot de variables categ√≥ricas con m√°s de dos categor√≠as\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
        "categorical_features = ['Region', 'IndRev_T01']\n",
        "X_encoded = encoder.fit_transform(X[categorical_features])\n",
        "\n",
        "# Combinar las caracter√≠sticas codificadas con el resto del DataFrame\n",
        "X_rest = X.drop(columns=categorical_features)\n",
        "X_final = pd.concat([pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(categorical_features)), X_rest.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# Verificaci√≥n del resultado\n",
        "print(\"Primeras filas del DataFrame codificado:\")\n",
        "print(X_final.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWyjyNCP9TNP"
      },
      "source": [
        "üîπTransformaci√≥n de Atributos para Modelos de Clasificaci√≥n\n",
        "\n",
        "Para adaptar el conjunto de datos a modelos de clasificaci√≥n, se realiz√≥ la transformaci√≥n de variables categ√≥ricas mediante codificaci√≥n One-Hot, asegurando que todas las columnas est√©n en formato num√©rico y apropiadas para aprendizaje supervisado.\n",
        "\n",
        "1. **Codificaci√≥n One-Hot**\n",
        "* **Objetivo**: Convertir variables categ√≥ricas como `Region` o `IndRev_T01` en un formato binario interpretable por modelos de machine learning.\n",
        "* **M√©todo**: Cada categor√≠a √∫nica se transforma en una columna binaria (`0` o `1`). Se utiliz√≥ `drop='first'` para evitar colinealidad entre variables generadas.\n",
        "* **Ejemplo de columnas resultantes**: `Region_2.0`, `Region_3.0`, etc., indican presencia de cada categor√≠a.\n",
        "\n",
        "2. **Integraci√≥n con el Dataset**\n",
        "* Las variables codificadas se combinaron con las variables num√©ricas originales y derivadas para conformar el conjunto final (`X_final`).\n",
        "* El resultado es una matriz de caracter√≠sticas puramente num√©rica, lista para alimentar modelos de clasificaci√≥n como regresi√≥n log√≠stica, √°rboles de decisi√≥n, o modelos basados en boosting.\n",
        "\n",
        "3. **Inspecci√≥n del Resultado**\n",
        "* Las primeras filas de `X_final` muestran la correcta conversi√≥n de variables categ√≥ricas y la conservaci√≥n de variables num√©ricas clave como Edad, Renta, Antig√ºedad, y diversos indicadores de actividad financiera.\n",
        "* El dataset resultante ofrece una representaci√≥n estructurada y compatible con t√©cnicas de modelado modernas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "T8bqQdbmdwhi",
        "outputId": "4894f20e-448a-4b76-a31b-8c508816c2e2"
      },
      "outputs": [],
      "source": [
        "# Visualizaci√≥n de la correlaci√≥n entre caracter√≠sticas seleccionadas\n",
        "\n",
        "# Selecci√≥n de caracter√≠sticas espec√≠ficas para la matriz de correlaci√≥n\n",
        "selected_features = ['Edad', 'Renta', 'Antiguedad', 'CUPO_L1', 'CUPO_L2', 'CUPO_MX']\n",
        "\n",
        "# Generar la matriz de correlaci√≥n solo para las caracter√≠sticas seleccionadas\n",
        "correlation_matrix = X[selected_features].corr()\n",
        "\n",
        "# Visualizar la matriz de correlaci√≥n\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", cbar_kws={'label': 'Correlaci√≥n'})\n",
        "plt.title(\"Matriz de Correlaci√≥n de Caracter√≠sticas Num√©ricas \")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx23WZJ4CTIe"
      },
      "source": [
        "üîπAn√°lisis de Correlaci√≥n entre Variables Num√©ricas\n",
        "\n",
        "La matriz de correlaci√≥n presentada analiza la relaci√≥n lineal entre seis variables clave: `Edad`, `Renta`, `Antig√ºedad`, `CUPO_L1`, `CUPO_L2` y `CUPO_MX`. Los coeficientes de correlaci√≥n oscilan entre -1 y 1, donde:\n",
        "\n",
        "* +1: Correlaci√≥n positiva perfecta.\n",
        "* -1: Correlaci√≥n negativa perfecta.\n",
        "* 0: Ausencia de correlaci√≥n lineal.\n",
        "\n",
        "Principales Hallazgos\n",
        "* **`CUPO_L1` vs. `CUPO_MX` (r = 0.66)**\n",
        "  * Existe una correlaci√≥n moderadamente alta entre el l√≠mite de compras nacionales (`CUPO_L1`) y el cupo internacional (`CUPO_MX`), lo que indica que ambos l√≠mites suelen escalar juntos, probablemente debido a pol√≠ticas crediticias internas.\n",
        "* **`CUPO_L1` vs. `Antig√ºedad` (r = 0.40)**\n",
        "  * Los clientes con mayor antig√ºedad tienden a tener un l√≠mite de cr√©dito m√°s alto, lo cual es consistente con pr√°cticas financieras donde la estabilidad en el tiempo es un factor de confianza.\n",
        "* **`Edad` vs. `Antig√ºedad` (r = 0.36) y `CUPO_L1` (r = 0.26)**\n",
        "  * La edad tiene una correlaci√≥n moderada con la antig√ºedad y leve con los l√≠mites de cr√©dito, sugiriendo cierta influencia del perfil etario, aunque no es un predictor dominante.\n",
        "* **`Renta` vs. `L√≠mites de Cr√©dito` (r ‚âà 0.25)**\n",
        "  * A pesar de ser una variable financiera clave, la renta muestra correlaciones relativamente bajas con los cupos de cr√©dito. Esto podr√≠a indicar que el otorgamiento de cupo considera otros factores como historial de pago o segmentaci√≥n comercial.\n",
        "* **Ausencia de Multicolinealidad Cr√≠tica**\n",
        "  * No se detectan correlaciones extremas (por encima de 0.8 o por debajo de -0.8), lo cual es positivo para el modelado. Las variables aportan informaci√≥n diferenciada y no redundante, reduciendo el riesgo de multicolinealidad.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hMyvmuNUY-p"
      },
      "source": [
        "### Random Forest + Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYsFdvxdh9x4",
        "outputId": "d9c32abf-8fd3-4350-93fb-5bc8564b2f93"
      },
      "outputs": [],
      "source": [
        "# Resumen de valores nulos antes del preprocesamiento\n",
        "print(\"Resumen de valores nulos antes del preprocesamiento:\")\n",
        "print(X.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMwk6UxfjH-a"
      },
      "source": [
        "üîπRevisi√≥n de Completitud de Datos\n",
        "\n",
        "Tras ejecutar una revisi√≥n posterior de los valores nulos en el conjunto de datos, se confirma que todas las columnas est√°n completamente pobladas. Variables clave como `Id`, `Subsegmento`, `Sexo`, `Region`, `Edad`, `UsoL1_T01`, `IndRev_T01` y `target` no presentan valores faltantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If_zw9lmZkoy"
      },
      "outputs": [],
      "source": [
        "# Definir las caracter√≠sticas num√©ricas y categ√≥ricas (modifica seg√∫n tus variables)\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Preprocesamiento para caracter√≠sticas num√©ricas (imputaci√≥n y escalado)\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Preprocesamiento para caracter√≠sticas categ√≥ricas (imputaci√≥n y codificaci√≥n)\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Crear el preprocesador usando ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Verificar y transformar `y` en una variable binaria si es continuo\n",
        "if y.dtype == 'float' or y.dtype == 'int':\n",
        "    threshold = np.median(y)\n",
        "    y = np.where(y >= threshold, 1, 0)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear el pipeline con el preprocesador y el modelo\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', LogisticRegression(max_iter=1000))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fstaa0JcaGXM",
        "outputId": "90744769-f195-4872-f1e3-8fb8a5d04e8f"
      },
      "outputs": [],
      "source": [
        "# Entrenamiento del modelo\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Evaluaci√≥n\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(\"Evaluaci√≥n del modelo (Random Forest + Pipeline):\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"Matriz de confusi√≥n:\\n\", confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX59SHC5aPy3",
        "outputId": "d9d1226e-df00-45bd-b3af-74c416b06d1d"
      },
      "outputs": [],
      "source": [
        "# Guardar el pipeline entero (preprocesamiento + modelo)\n",
        "joblib.dump(pipeline, 'models/modelo_pipeline_rf.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv3MNqwxaRuz"
      },
      "outputs": [],
      "source": [
        "# Cargar modelo\n",
        "modelo_cargado = joblib.load('models/modelo_pipeline_rf.pkl')\n",
        "\n",
        "# Predecir directamente con el pipeline\n",
        "y_pred_cargado = modelo_cargado.predict(X_test)\n",
        "\n",
        "# Evaluaci√≥n para verificar consistencia\n",
        "print(\"Evaluaci√≥n del modelo cargado:\")\n",
        "print(classification_report(y_test, y_pred_cargado, digits=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4l2l5A4nFdb"
      },
      "source": [
        "üîπEvaluaci√≥n del Modelo: Exactitud e Implicaciones\n",
        "\n",
        "* **Exactitud Obtenida**\n",
        "  * El modelo alcanz√≥ una exactitud del 96%, lo cual representa un rendimiento sobresaliente considerando las variables utilizadas. Este nivel de desempe√±o sugiere que las caracter√≠sticas disponibles en el dataset son efectivas para capturar los patrones necesarios para la clasificaci√≥n.\n",
        "* **Interpretaci√≥n y Consideraciones para Mejora**\n",
        "  * La exclusi√≥n de ciertas columnas no afect√≥ negativamente el rendimiento del modelo, lo que indica que dichas variables probablemente no aportaban informaci√≥n cr√≠tica en su estado actual.\n",
        "  * No obstante, si estas columnas contienen informaci√≥n futura o si se reduce el sesgo de disponibilidad (por ejemplo, completando valores faltantes con estrategias robustas), podr√≠an evaluarse nuevamente para determinar si contribuyen a mejorar la capacidad predictiva.\n",
        "  * En el caso de variables categ√≥ricas, una estrategia m√°s s√≥lida que la eliminaci√≥n podr√≠a ser asignar una categor√≠a expl√≠cita para valores faltantes (como \"Desconocido\"), preservando la informaci√≥n estructural del dataset sin introducir ruido estad√≠stico innecesario.\n",
        "\n",
        "üîπConclusi√≥n\n",
        "\n",
        "El modelo demostr√≥ ser eficaz incluso con un subconjunto reducido de variables, lo cual refuerza su robustez y eficiencia. Sin embargo, se recomienda seguir evaluando el valor agregado de variables omitidas a medida que se disponga de datos m√°s completos.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hGpwMKkf6DQ"
      },
      "source": [
        "**Reconocer Overfitting y Underfitting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWwjyYXVgAFP",
        "outputId": "60579bac-5872-49df-a7f2-b2d0615c3780"
      },
      "outputs": [],
      "source": [
        "# Calcular la exactitud en el conjunto de entrenamiento y en el conjunto de prueba\n",
        "train_accuracy = accuracy_score(y_train, pipeline.predict(X_train))\n",
        "test_accuracy = accuracy_score(y_test, pipeline.predict(X_test))\n",
        "\n",
        "print(f\"Exactitud en entrenamiento: {train_accuracy:.2f}\")\n",
        "print(f\"Exactitud en prueba: {test_accuracy:.2f}\")\n",
        "\n",
        "# Interpretaci√≥n de overfitting o underfitting\n",
        "if train_accuracy > test_accuracy + 0.1:\n",
        "    print(\"Posible overfitting detectado: el modelo tiene un desempe√±o mucho mejor en entrenamiento que en prueba.\")\n",
        "elif test_accuracy > train_accuracy:\n",
        "    print(\"Posible underfitting detectado: el modelo no est√° capturando patrones suficientemente bien.\")\n",
        "else:\n",
        "    print(\"Buen ajuste: el modelo tiene un desempe√±o balanceado entre entrenamiento y prueba.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJx2A-4km6LN",
        "outputId": "0eda80d6-b6d2-4fea-8ab9-7b086b7d5da5"
      },
      "outputs": [],
      "source": [
        "# Calcular la exactitud en el conjunto de entrenamiento y en el conjunto de prueba\n",
        "train_accuracy = accuracy_score(y_train, pipeline.predict(X_train))\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Exactitud en entrenamiento: {train_accuracy:.2f}\")\n",
        "print(f\"Exactitud en prueba: {test_accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLOa6GHKqgyI"
      },
      "source": [
        "üîπEvaluaci√≥n del Ajuste del Modelo: Overfitting vs. Underfitting\n",
        "\n",
        "* **An√°lisis del Desempe√±o**:\n",
        "  * Para evaluar el comportamiento del modelo, se compar√≥ su rendimiento en los conjuntos de entrenamiento y prueba utilizando la m√©trica de exactitud (accuracy):\n",
        "    * Exactitud en entrenamiento: 97%\n",
        "    * Exactitud en prueba: 96%\n",
        "\n",
        "* **Interpretaci√≥n**:\n",
        "  * La diferencia marginal entre ambos conjuntos sugiere que el modelo generaliza adecuadamente y no presenta overfitting (sobreajuste).\n",
        "  * Asimismo, dado que ambos resultados son altos, el modelo tampoco evidencia underfitting (subajuste), es decir, logra captar patrones relevantes sin sobreajustarse a los datos de entrenamiento.\n",
        "\n",
        "* **Mensajes de Advertencia**:\n",
        "  * Se emitieron advertencias relacionadas con columnas que no contienen valores observados (como 'Sexo' o 'IndRev_T12'). Estas advertencias se refieren a omisiones autom√°ticas en el proceso de imputaci√≥n, pero no impactan negativamente el rendimiento del modelo, ya que dichas columnas no conten√≠an informaci√≥n √∫til o estaban correctamente gestionadas en etapas anteriores del preprocesamiento.\n",
        "\n",
        "üîπConclusi√≥n\n",
        "\n",
        "El modelo logra un buen equilibrio entre sesgo y varianza, lo que lo convierte en una base s√≥lida para aplicaciones predictivas con los datos actuales. No obstante, es recomendable seguir monitoreando este equilibrio si se incorporan nuevas variables o si se modifica la estrategia de imputaci√≥n.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEvl7YgfuVz2"
      },
      "source": [
        "**M√©tricas de Evaluaci√≥n de Modelos de Clasificaci√≥n**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjzr5kXsoC4W",
        "outputId": "3dc4e14c-3433-4c49-ce0f-6f67f7c62f83"
      },
      "outputs": [],
      "source": [
        "# Reporte de clasificaci√≥n detallado\n",
        "print(\"Reporte de Clasificaci√≥n:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-ROH3rdufy_"
      },
      "source": [
        "üîπEvaluaci√≥n del Modelo: M√©tricas de Clasificaci√≥n\n",
        "\n",
        "Se evalu√≥ el rendimiento del modelo utilizando las m√©tricas est√°ndar: precisi√≥n, recall, F1-score y exactitud, aplicadas a cada clase del target.\n",
        "\n",
        "üîπResultados principales\n",
        "\n",
        "| Clase    | Precision | Recall | F1-score | Soporte |\n",
        "| -------- | --------- | ------ | -------- | ------- |\n",
        "| 0        | 0.96      | 0.96   | 0.96     | 2716    |\n",
        "| 1        | 0.96      | 0.96   | 0.96     | 2950    |\n",
        "| Accuracy |           |        | 0.96     | 5666    |\n",
        "\n",
        "* **Macro Avg (no ponderado)**: Todas las m√©tricas promedian 0.96, lo que indica un desempe√±o parejo entre clases.\n",
        "* **Weighted Avg (ponderado)**: Igualmente 0.96, confirmando que el modelo mantiene alta precisi√≥n incluso considerando la proporci√≥n de cada clase.\n",
        "\n",
        "üîπInterpretaci√≥n\n",
        "\n",
        "* **Rendimiento equilibrado**: El modelo clasifica con la misma eficacia ambas clases, lo que es ideal para tareas donde ambas categor√≠as tienen valor estrat√©gico.\n",
        "* *No se requieren ajustes adicionales¬®**: Dado el balance en el desempe√±o y la distribuci√≥n relativamente pareja del soporte entre clases, no se observan problemas de desbalance ni necesidad de t√©cnicas de reponderaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "dSpDWvqtoO5B",
        "outputId": "3722b763-7395-407b-cddc-079b45b5fe7a"
      },
      "outputs": [],
      "source": [
        "# Generar matriz de confusi√≥n\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualizar la matriz de confusi√≥n\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel(\"Predicci√≥n\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.title(\"Matriz de Confusi√≥n\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc0iZw3so2Qu"
      },
      "source": [
        "üîπAn√°lisis de la Matriz de Confusi√≥n\n",
        "\n",
        "|        | Predicho 0 | Predicho 1 |\n",
        "| ------ | ---------- | ---------- |\n",
        "| Real 0 | 2608 (VN)  | 108 (FP)   |\n",
        "| Real 1 | 119 (FN)   | 2831 (VP)  |\n",
        "\n",
        "Interpretaci√≥n\n",
        "\n",
        "* **Precisi√≥n elevada en ambas clases**: El modelo clasifica correctamente el 96% de los casos de cada clase, con un buen balance entre Verdaderos Positivos (2831) y Verdaderos Negativos (2608).\n",
        "\n",
        "* **Errores moderados**:\n",
        "  * **Falsos Positivos (108)**: Casos de clase 0 incorrectamente clasificados como 1.\n",
        "  * **Falsos Negativos (119)**: Casos de clase 1 predichos como 0.\n",
        "  * Estos errores son proporcionales y no generan sesgo significativo en el modelo.\n",
        "* **Balance de clases**: No se observa una desproporci√≥n cr√≠tica entre clases (2716 vs. 2950), por lo que no es estrictamente necesario aplicar t√©cnicas de balanceo (como resampling o class weighting). Sin embargo, podr√≠an explorarse en una etapa de refinamiento para optimizar la sensibilidad o reducir falsos negativos, dependiendo del costo de error para el negocio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusi√≥n Final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Este proyecto demostr√≥ un enfoque completo y estructurado para el an√°lisis y modelado de datos reales del sector financiero. A trav√©s de un proceso de limpieza, transformaci√≥n y evaluaci√≥n de m√∫ltiples modelos, se lograron los siguientes resultados clave:\n",
        "\n",
        "* **Calidad del dataset**: Se logr√≥ una base de datos limpia, sin valores nulos, con variables transformadas adecuadamente y codificadas para el modelado.\n",
        "* **An√°lisis exploratorio**: Identific√≥ patrones relevantes, como la alta concentraci√≥n de clientes en ciertos segmentos (Regi√≥n 13, subsegmento 170), y un uso predominantemente bajo de productos financieros.\n",
        "* **Modelado de regresi√≥n**: El modelo de regresi√≥n lineal aplicado al target transformado (log(1 + transacciones anuales)) logr√≥ un desempe√±o aceptable (MSE: 0.31, RMSE: 0.55), aunque se reconoci√≥ que no era ideal para producci√≥n.\n",
        "* **Modelado de clasificaci√≥n**: Se desarroll√≥ un modelo robusto de clasificaci√≥n binaria para la retenci√≥n de clientes, alcanzando una exactitud del 96%, sin se√±ales de overfitting ni underfitting. La matriz de confusi√≥n y m√©tricas como precisi√≥n, recall y F1-score confirmaron un desempe√±o equilibrado entre clases."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
